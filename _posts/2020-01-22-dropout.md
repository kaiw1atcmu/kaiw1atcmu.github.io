---
title: Dropout [more to come]
permalink: dropout.html
date: 2020-01-22
tags: [deep_learning]
published: false
---

The dropout technique was proposed by Hinton et al. (2012) as a form of regularization for fully connected neural
network layers. More recently, the DropConnect technique, a generalization of dropout for regularizing large
fully-connected layers within neural networks, was proposed by LeCun et al. (2013). In this post, we are trying to give
ourselves an elementary guide through the dropout technique.

## Dropout
With unlimited computation, the best way to "regularize" a fixed-sized model is to average the predictions of all
possible settings of the parameters, weighting each setting by its posterior probability given the training data, the
approach that is taken by Bayesian neural networks. However, considerations must be taken into account as neural
networks grow larger, since the simple model ensemble method is prohibitive in terms of computational resources and
available training data.

Dropout is a technique that efficiently addresses both these issues, which approximately results in an equally weighted,
geometrically averaged model prediction over the exponentially vast number of possible weight-sharing network
structures. The idea of dropout is not limited to feed-forward neural nets. It can be more generally applied to
graphical models such as restricted Boltzmann Machines, and recurrent neural networks. Interested readers are encouraged
to further refer to [Learning stochastic recurrent networks](#references) and
[How to construct deep recurrent neural networks](#references).

One of the drawbacks of dropout is that it increases training time. A dropout network typically takes $2$-$3$ times
longer to train than a standard neural network of the same architecture. Because dropout is a regularization technique,
it reduces the effective capacity of a model. To offset this effect, we must increase the size of the model. Typically
the optimal validation set error is much lower when using dropout, but this comes at the cost of a much larger model and
many more iterations of the training algorithm. For very large datasets, regularization confers little reduction in
generalization error. In these cases, the computational cost of using dropout and larger models may outweigh the benefit
of regularization. When extremely few labeled training examples are available, dropout is less effective. Bayesian
neural networks outperform dropout on the Alternative Splicing Dataset where fewer than $5000$ examples are available.
When additional unlabeled data is available, unsupervised feature learning can gain an advantage over dropout.

<img src="{{ "images/20200203-1.png" }}" alt="dropout structure"/>
_Figure 1: Dropout structure. By courtesy of [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](#references)_

## Training and Test with Dropout
At training time, a unit is present with probability of retention $p$ and is connected to units in the next layer with
weights $\mathbf{w}$. This procedure creates an ensemble of thinned networks consists of all the units that survived
dropout. At test time, the unit is always present and the outgoing weights $\mathbf{w}$ of that unit are multiplied by
$p$. This ensures that for any hidden unit the expected output (under the distribution used to drop units at training
time) is the same as the actual output at test time. We call this approach the **weight scaling inference rule**.
Another way to achieve the same effect is to scale up the retained activations by multiplying by $1/p$ at training time
and not modifying the weights at test time. These methods are equivalent with appropriate scaling of the learning rate
and weight initializations at each layer.

An alternative method for inference is the Monte Carlo model average. Although Ian Goodfellow found the weight scaling
approximation works better than Monte Carlo approximations with up to $1000$ sub-networks, other authors achieved better
results with the Monte Carlo approximation. It appears that the optimal choice of inference approximation is
problem-dependent.

No theoretical guidance exists concerning the optimal choice of probability of retention $p$. As a rule of thumb,
however, dropping out $20$% of the input units and $50$% of the hidden units was often found to be optimal for a wide
range of networks and tasks.

<img src="{{ "images/20200203-2.png" }}" alt="dropout training"/>
_Figure 2: Dropout at training and test time. By courtesy of [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](#references)_

## Backpropagation
Dropout neural networks can be trained using stochastic gradient descent in a manner similar to standard neural nets.
The gradients for each parameter are averaged over the training cases in each mini-batch. Although dropout alone gives
significant improvements, using dropout along with max-norm regularization, large decaying learning rates and high
momentum provides a significant boost over just using dropout. more to come...

## Pretraining
Neural networks can be pretrained using stacks of RBMs, autoencoders or deep Boltzmann machines. Pretraining is an
effective way of making use of unlabeled data. Dropout can be applied to finetune nets that have been pretrained using
these techniques. The pretraining procedure stays the same. The weights obtained from pretraining should be scaled up by
a factor of $1/p$. This makes sure that for each unit, the expected output from it under random dropout will be the same
as the output during pretraining. more to come...

## Comparison with Techniques by Adding Noise
Dropout can be interpreted as a way of regularizing a neural network by adding noise to its hidden units. The idea of
adding noise to the states of units has previously been used in the context of **Denoising Autoencoders (DAEs)** where
noise is added to the input units. Experiments showed that adding noise is not only useful for unsupervised feature
learning but can also be extended to supervised learning problems. Experiments also showed that the weight scaling
procedure enables dropout to use much higher noise levels than DAEs. Interested readers are encouraged to refer to
[denoising autoencoders](#references) and [stacked denoising autoencoders](#references). more to come...

## Comparison with Techniques by Marginalizing Noise
Since dropout can be seen as a stochastic regularization technique, it is natural to consider its deterministic
counterpart which is obtained by marginalizing out the noise. The authors showed that, in simple cases, dropout can be
analytically marginalized out to obtain deterministic regularization methods. Recent researches explored noise
marginalization under restricted models, mostly concerned with noisy inputs only, and no hidden layers. more to come...

## Comparison with Bayesian Neural Networks
On the other hand, Bayesian neural networks by Neal (1996) are the proper way of doing model averaging over the space of
neural network structures and parameters. In dropout, each model is weighted equally, whereas in a Bayesian neural
network each model is weighted taking into account the prior and how well the model fits the data, which is the more
correct approach. Bayesian neural nets are extremely useful for solving problems in domains where data is scarce such as
medical diagnosis, genetics, drug discovery and other computational biology applications. more to come...

## Multiplicative Gaussian Noise
Dropout involves multiplying hidden activations by Bernoulli distributed random variables which take the value $1$ with
probability $p$ and $0$ otherwise. This idea can be generalized by multiplying the activations with random variables
drawn from other distributions. The authors recently discovered that multiplying by a random variable drawn from
$\mathcal{N}(1,1)$ works just as well, or perhaps better than using Bernoulli noise.

## Remarks
Ian Goodfellow provided a couple of key insights into dropout. Dropout prevents co-adaptation by making the presence of
other hidden units unreliable, so that each hidden unit on its own seems to be detecting a meaningful feature, which is
probably the main reason why it leads to lower generalization errors. The authors also found a side-effect of doing
dropout, that is, the activations of the hidden units become sparse, even when no sparsity inducing regularizers are
present.

The stochasticity used while training with dropout is not necessary for the approach's success, and it is also not
sufficient. more to come...

Traditional noise injection techniques that add unstructured noise at the input are not able to randomly erase a feature
extracted by a hidden unit, whereas destroying extracted features rather than original values forces the model to learn
other features, and use of all of the knowledge about the input distribution that the model has acquired so far.

Another important aspect of dropout is that the noise is multiplicative. If the noise were additive with fixed scale,
then a rectified linear hidden unit with added noise could simply learn to have its output become very large in order to
make the added noise insignificant by comparison. Multiplicative noise does not allow such a pathological solution to
the noise robustness problem.

Another deep learning algorithm, batch normalization, reparametrizes the model in a way that introduces both additive
and multiplicative noise on the hidden units at training time. The primary purpose of batch normalization is to improve
optimization, but the noise can have a regularizing effect, and sometimes makes dropout unnecessary.

## Further Reading
P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. 2008. Extracting and composing robust features with denoising
autoencoders. In ICML 2008.

P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. 2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local denoising criterion. In ICML 2010.

Warde-Farley, D., Goodfellow, I. J., Courville, A., and Bengio, Y. 2014. An empirical analysis of dropout in piecewise
linear networks. In ICLR'2014.

Bayer, J. and Osendorfer, C. 2014. Learning stochastic recurrent networks. ArXiv e-prints.

Pascanu, R., Gülçehre, Ç., Cho, K., and Bengio, Y. 2014. How to construct deep recurrent neural networks. In ICLR'2014.

## References
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov. 2014. Dropout: A Simple Way
to Prevent Neural Networks from Overfitting. In JMLR 2014.

{% include links.html %}
