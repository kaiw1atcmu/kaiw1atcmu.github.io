---
title: "Expectation Maximization and Beyond"
keywords: expectation maximization
date: 2020-01-02 17:32:35 -0800
last_updated: December 26, 2020
tags: [statistics, machine_learning]
summary: "In this post, let us discuss a classical algorithm in statistics, i.e. the expectation maximization (EM) and
its variants, e.g. generalized expectation maximization (GEM)."
sidebar: mydoc_sidebar
permalink: expectation_maximization_and_beyond.html
folder: mydoc
---

Suppose we encounter cases in which the available data set is incomplete, i.e., with missing attributes $\mathcal{Z}$
or missing label $\mathcal{Y}$ besides observed attributes $\mathcal{X}$. Bearing the maximum likelihood principle
in mind, we seek to have the log-likelihood function $\log\ p(\mathbf{x}\vert\mathbf{\theta})$, or the **evidence
function**, maximized over $\mathbf{\theta}$, a problem that often proves difficult to approach directly.

However, we could bypass direct optimization efforts and consider the **expectation maximization (EM)** algorithm. In
general, the EM algorithm maximizes the expectation of the log-likelihood function, conditioned on the observed samples
and $\mathbf{\theta}(t)$, the current iterative estimate of $\mathbf{\theta}$. It can be shown that the successive
estimates $\mathbf{\theta}(t)$ never decrease the likelihood function (of observed data given $\mathbf{\theta}(t)$). The
likelihood function (of observed data given $\mathbf{\theta}(t)$) keeps non-decreasing until a maximum (local or global)
is reached and the EM algorithm converges.

## The Expectation Maximization (EM) Algorithm
Let us first state the situation in more general terms. For the **maximum likelihood estimate (MLE)**, we are trying to
find the optimal parameter vector $\mathbf{\theta}$ which maximizes the likelihood function
$\log p(\mathbf{x}\vert\mathbf{\theta})$. However, in the scenario where observed data are incomplete, we have
unobserved data $\mathcal{Z}$ in addition to observed ones $\mathcal{X}$. Instead of directly maximizing
$\log p(\mathbf{x}\vert\mathbf{\theta})$, we could try to maximize the expected likelihood function given by,

$$
  {\int_{\mathbf{z}}\log p(\mathbf{x},\mathbf{z}\vert\mathbf{\theta})p(\mathbf{z}\vert\mathbf{x},\mathbf{\theta})d\mathbf{z},}
$$
    
the expectation of $\log p(\mathbf{x},\mathbf{z}\vert\mathbf{\theta})$ with respect to the posterior probability of
$\mathcal{Z}$. The rationale behind is that since $\mathcal{X}$ and $\mathcal{Z}$ are both data generated by the
parameter vector $\mathbf{\theta}$ (no matter whether we can have it observed), it makes more sense that an optimal
choice of $\mathbf{\theta}$ should maximize the global likelihood function
$\log p(\mathbf{x},\mathbf{z}\vert\mathbf{\theta})$. With unobserved data $\mathbf{z}$ a more tractable version is its
expectation with respect to posterior probability of $\mathbf{z}$, which is the aforementioned expected likelihood
function.

To facilitate computation, we define an auxiliary function $Q(\mathbf{\theta};\mathbf{\theta'})$ to be:

$$
  {Q(\mathbf{\theta};\mathbf{\theta'})=\int_{\mathbf{z}}\log p(\mathbf{x},\mathbf{z};\mathbf{\theta})p(\mathbf{z}\vert\mathbf{x};\mathbf{\theta'})d\mathbf{z},}
$$

the E-step to be:

$$
  {Q(\mathbf{\theta};\mathbf{\theta}(t))=\int_{\mathbf{z}}\log p(\mathbf{x},\mathbf{z};\mathbf{\theta})p(\mathbf{z}\vert\mathbf{x};\mathbf{\theta}(t))d\mathbf{z},}
$$

and the M-step to be:

$$
  {\mathbf{\theta}(t+1):\ \frac{\partial Q(\mathbf{\theta};\mathbf{\theta}(t))}{\partial \mathbf{\theta}}\bigg\vert_{\mathbf{\theta}=\mathbf{\theta}(t+1)}=\mathbf{0}.}
$$

To see the relation of $Q(\mathbf{\theta};\mathbf{\theta'})$ to
$\int_{\mathbf{z}}\log p(\mathbf{x},\mathbf{z}\vert\mathbf{\theta})p(\mathbf{z}\vert\mathbf{x},\mathbf{\theta})d\mathbf{z}$,
we should notice that as $\mathbf{\theta}(t)$ approaches its true value $\mathbf{\theta}$, in the M-step
$Q(\mathbf{\theta}(t+1);\mathbf{\theta}(t))$ approaches
$\int_{\mathbf{z}}\log p(\mathbf{x},\mathbf{z}\vert\mathbf{\theta})p(\mathbf{z}\vert\mathbf{x},\mathbf{\theta})d\mathbf{z}$
as expected.

Replace integral over $\mathcal{Z}$ with summation if $\mathcal{Z}$ is a discrete-valued variable. Starting with an
initial choice of $\mathbf{\theta}=\mathbf{\theta}(0)$, the EM algorithm successively alternates between E-steps and
M-steps to have updated value of $\mathbf{\theta}(t)$ until convergence is reached or some other stopping criterion is
satisfied.

## Mathematical Proofs
Using conditional probability formula to decompose $Q(\mathbf{\theta}(t+1);\mathbf{\theta}(t))$ into two summands,

$$
  {Q(\mathbf{\theta}(t+1);\mathbf{\theta}(t))=\int_{\mathbf{z}}\log p(\mathbf{x},\mathbf{z};\mathbf{\theta}(t+1))p(\mathbf{z};\mathbf{x},\mathbf{\theta}(t))d\mathbf{z}}\\
  {=\int_{\mathbf{z}}(\log p(\mathbf{z}\vert\mathbf{x};\mathbf{\theta}(t+1))+\log p(\mathbf{x};\mathbf{\theta}(t+1)))p(\mathbf{z}\vert\mathbf{x};\mathbf{\theta}(t))d\mathbf{z}}\\
  {=\int_{\mathbf{z}}\log p(\mathbf{z}\vert\mathbf{x};\mathbf{\theta}(t+1))p(\mathbf{z}\vert\mathbf{x};\mathbf{\theta}(t))d\mathbf{z}+\log p(\mathbf{x};\mathbf{\theta}(t)),}
$$

and according to M-step which solves for optimal $\mathbf{\theta}(t+1)$ given $\mathbf{\theta}(t)$, we must have
    
$$
  {Q(\mathbf{\theta}(t+1);\mathbf{\theta}(t))\geq Q(\mathbf{\theta}(t);\mathbf{\theta}(t)).}
$$

By a variant of **Jensen's inequality**,
    
$$
  {\int_{\mathbf{z}}\log\frac{p(\mathbf{z}\vert\mathbf{x};\mathbf{\theta}(t+1))}{p(\mathbf{z}\vert\mathbf{x};\mathbf{\theta}(t))}p(\mathbf{z}\vert\mathbf{x};\mathbf{\theta}(t))d\mathbf{z}\leq 0,}
$$

it follows that

$$
  {\log p(\mathbf{x};\mathbf{\theta}(t+1))\geq\log p(\mathbf{x};\mathbf{\theta}(t)),}
$$

which means $\log p(\mathbf{x};\mathbf{\theta}(t))$, the log probability of the observed data, or the **evidence
function**, never decreases with index $t$. Under some mild conditions, such as $p(\mathbf{x};\mathbf{\theta}(t))$ is
upper bounded by a positive value, $p(\mathbf{x};\mathbf{\theta}(t))$ is guaranteed to converge, but not necessarily to
its global maximum.

## Relation to the $F$ Function
[Neal and Hinton (1999)](#references) introduced the $F$ function and established its relation to the EM algorithm in a
discussion on its properties. Suppose the probability density function of unobserved data $\mathcal{Z}$ is
$q(\mathbf{z})$. Let us define the bivariate $F$ function in $q(\mathbf{z})$ and $\mathbf{\theta}$ to be

$$
  {F(q(\mathbf{z}),\mathbf{\theta})=\mathbb{E}_{q(\mathbf{z})}[\log p(\mathbf{x},\mathbf{z}\vert\mathbf{\theta})]
  +\mathbb{E}_{q(\mathbf{z})}[-\log q(\mathbf{z})],}
$$

in which we suppose $p(\mathbf{x},\mathbf{z}\vert\mathbf{\theta})$ is a continuous function in $\mathbf{\theta}$.
Therefore the $F$ function is continuous in $q(\mathbf{z})$ and $\mathbf{\theta}$. For an arbitrary $\mathbf{\theta}$,
with Lagrangian multipliers for constrained optimization and calculus of variation, the maximizer of the $F$ function is
$q_{\mathbf{\theta}}(\mathbf{z})=p(\mathbf{z}\vert\mathbf{x},\mathbf{\theta})$, and of course it is continuous in
$\mathbf{\theta}$. Some basic algebra reveals that $F$ attains its maximum with this choice of
$q_{\mathbf{\theta}}(\mathbf{z})$,

$$
  {F(q_{\mathbf{\theta}}(\mathbf{z}),\mathbf{\theta})=\log p(\mathbf{x}\vert\mathbf{\theta}).}
$$

One can verify that EM is equivalent to the maximization-maximization algorithm on the $F$ function. That is,
alternating between the two steps until convergence is reached or some other stopping criterion is satisfied,

$$
  {\textrm{for }{\mathbf{\theta}}^{(i)}\textrm{, find the maximizer }q(\mathbf{z})^{(i+1)}\textrm{ of }F(q(\mathbf{z}),\mathbf{\theta}^{(i)}),}\\
  {\textrm{for }q(\mathbf{z})^{(i+1)}\textrm{, find the maximizer }\mathbf{\theta}^{(i+1)}\textrm{ of }F(q(\mathbf{z})^{(i+1)},\mathbf{\theta}).}
$$

Given an initial $\mathbf{\theta}=\mathbf{\theta}^{(0)}$, this procedure is shown to generate in an identical series of
$\mathbf{\theta}^{(i)}$ to that resulted from the EM algorithm.

## The Generalized Expectation Maximization (GEM) Algorithm
The **generalized expectation maximization (GEM)** algorithm is a bit more lax than the EM algorithm, and require merely
that an improved $\mathbf{\theta}^{(t+1)}$ be set in the M-step of the algorithm, not necessarily the optimal.
Naturally, convergence will not be as rapid as for a proper EM algorithm, but GEM algorithms afford greater freedom to
choose computationally simpler steps.

## Remarks on the EM Algorithm
In contrast with other optimization procedures, the expectation-maximization algorithm has properties including:

* Smooth and stable. However, the great advantage of the algorithm is that its convergence is smooth and is not
vulnerable to instabilities.

* Less computation. Furthermore, it is computationally more attractive than Newton-like methods, which require the
computation of the Hessian matrix.

* Slow convergence. Theoretical results as well as practical experimentation confirm that the convergence is slower than
the quadratic convergence of Newton-type searching algorithms, although near the optimum a speedup may be possible.

* No Global Maximum Guarantee. Even in the case of convergence, $\mathbf{\theta}$ is not guaranteed to converge to
$\arg\max_{\theta}\int_{\mathbf{z}}\log p(\mathbf{x},\mathbf{z}\vert\mathbf{\theta})p(\mathbf{z}\vert\mathbf{x},\mathbf{\theta})d\mathbf{z}$.

## References
Wu, C. 1982. On the Convergence Properties of the EM Algorithm. The Annals of Statistics.

Neal, R. and Hinton, G. 1999. A view of the EM algorithm that justifies incremental, sparse, and other variants. In
M. I. Jordan, editor, Learning in Graphical Models.

{% include links.html %}
