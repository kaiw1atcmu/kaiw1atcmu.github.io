---
title: "Decision Tree Pruning"
keywords: decision tree, tree pruning
date: 2019-11-29 13:25:52 -0800
last_updated: December 26, 2020
tags: [machine_learning]
summary: "Decision tree pruning is a key technique to prevent overfitting and improve generalization performance on
unseen data. There are several approaches and their variants to tree pruning in decision tree learning."
sidebar: none
permalink: decision_tree_pruning.html
folder: mydoc
---

Decision tree pruning is a key technique to prevent overfitting and improve generalization performance on unseen data.
There are several approaches and their variants to tree pruning in decision tree learning.

<center>
    <img src="{{ "images/20191129-1.jpeg" }}" alt="decision tree learning"/>
    <font face="Lora">Figure 1: An Illustrative Example Of General Decision Tree Learning.</font>
</center>

Broadly speaking, these approaches can be grouped into two categories: 1) approaches that stop growing the tree earlier,
before it reaches the point where it perfectly classifies the training data, and 2) approaches that allow the tree to
overfit the data, and then post-prune the tree. Although the first of these approaches might seem more direct, the
second approach of post-pruning overfit trees has been found to be more successful in practice. This is due to the
difficulty in the first approach of estimating precisely when to stop growing the tree.

## Prepruning
> **algorithm**: Prepruning  
> **function**: $\text{TreeGenerate}(D,V,A)$  
> **input**: add a validation set $V$  
> ...  
> (after each splitting)  
> **if** splitting on attribute $a\in A$ that best classifies $D$ improves performance in $V$  
> &emsp;&emsp; split on attribute $a\in A$  
> **else**  
> &emsp;&emsp; do not split  
> **end if**  
> an example in [Zhihua Zhou (2016)](#references) does not try splitting on other sub-optimal attributes)  
> ...

The Prepruning algorithm precludes the formation of many subtrees, and constructs a decision tree that is significantly
shorter in height than without Prepruning. However, decision trees resulted from this procedure suffer from the risk of
underfitting instead.

## Reduced-Error Pruning
> **algorithm**: Reduced-Error Pruning  
> **function**: $\text{TreeGenerate}(D,V,A)$  
> **input**: add a validation set $V$  
> ...  
> (after tree formation)  
> **while** not all $parent$ nodes whose children are leafs are tested (including $parent$ generated by pruning)  
> &emsp;&emsp; (an example in [Zhihua Zhou (2016)](#references) does not try $parent$ with non-leaf children)  
> &emsp;&emsp; <b>if</b> making $parent$ a leaf and assigning it the majority class affiliated with it does not worsen
performance in $V$  
> &emsp;&emsp;&emsp;&emsp; make $parent$ a leaf node and assign it the majority class affiliated with it  
> &emsp;&emsp; **end if**  
> **end while**  
> ...

The Post-Pruning algorithm generally allows the formation of more subtrees than Prepruning. This pruning procedure runs
a much lower risk of underfitting, and generalizes better than Prepruning.

## Rule Post-Pruning
In practice, one quite successful method for finding high accuracy hypotheses is a technique we shall call Rule
Post-Pruning. A variant of this pruning method is used by C4.5, which is an outgrowth of the original ID3 algorithm.
Rule post-pruning involves the following steps:

> Infer the decision tree from the training set, growing the tree until the training data is fit as well as possible and
allowing overfitting to occur.
>  
> Convert the learned tree into an equivalent set of rules by creating one rule for each path from the root node to a
leaf node.
>
> Prune (generalize) each rule by removing any preconditions that result in improving its estimated accuracy.
>
> Sort the pruned rules by their estimated accuracy, and consider them in this sequence when classifying subsequent
instances.

Besides using an independent test set to estimate the rule accuracy, another method, used by C4.5, is to evaluate
performance based on the training set itself, and calculates the lower-bound of the estimated accuracy's confidence
interval, as a pessimistic estimate. Although this heuristic method is not statistically valid, it has nevertheless been
found useful in practice for large data sets.

## CART Pruning
Previous decision tree pruning techniques use a separate validation set to avoid overfitting and improve generalization.
Instead, the cost function associated with CART tree pruning incorporates the structural risk of tree size, in addition
to the empirical risk, imposing an explicit penalty on model complexity. Then the total cost function of tree $T$ with
the weighting hyperparameter $\alpha$ becomes

$$
  {C_\alpha(T)=C(T)+\alpha\vert T\vert,\ \alpha\geq0.}
$$

Breiman et al. proposed an iterative and greedy (i.e., without backtracking) pruning algorithm that efficiently solves
the optimal tree pruning problem. They also showed that given a preselected $\alpha\geq0$, the optimally pruned tree is
unique in the sense that a decision tree of smaller size $\vert T\vert$ is preferable to one of larger size, when both
trees have equivalent cost function values. To be specific, the algorithm involves defining a quantity $g(t)$ associated
with each node $t$ in $T$, given by

$$
  {g(t)=\frac{C(t)-C(T_t)}{\vert T_t\vert-1},}
$$

where $C(t)$ corresponds to the empirical risk of the single node $t$, and $C(T_t)$ corresponds to the empirical risk of
the subtree rooted at node $t$. Starting from $\alpha_0=0$, where the structural risk plays no part in the cost
function, the optimal decision tree $T_0$ is the entire tree $T$ itself. The CART pruning algorithm iterates over the
following steps, until the current pruned tree $T_k$ shrinks to a single-node decision tree stump:

> For the current pruned tree $T_k$, examine all nodes $t$ in $T_k$ and select the node(s) having the lowest $g(t)$.
>
> Prune this node (set of nodes) and assign class labels to its (their) associated training samples by majority vote,
resulting in pruned tree $T_{k+1}$. Set $\alpha_{k+1}=\min_{t\in T_k}g(t)$.
>
> The optimally pruned decision tree for hyperparameter $\alpha\in[\alpha_k,\alpha_{k+1})$ is $T_k$. Store $T_k$ with
interval $[\alpha_k,\alpha_{k+1})$, and increment $k$ for the next iteration.

Clearly, the single-node decision tree stump $T_n$ corresponds to hyperparameter $\alpha\in[\alpha_n,+\infty)$. In
summarizing, optimally pruned decision trees $\{T_0,T_1,\ldots,T_n\}$ corresponds to hyperparameters $\alpha$ taken from
intervals $[0,\alpha_1),[\alpha_1,\alpha_2),\ldots,[\alpha_n,+\infty)$, and the pruned trees constitute a sequence where
$T_{k+1}$ consists of a proper subset of nodes in its predecessor, $T_k$. If it were not the case, pruning the set of
nodes in $T_{k+1}/T_k$ would always result in an improved cost function value for hyperparameters
$\alpha\geq\alpha_{k+1}$. Likewise, if $\alpha_{k+1}=\min_{t\in T_k}g(t)$ were not larger than $\alpha_k$, we would
always be better off by pruning a node different than, or being ancestral of the node pruned in generating $T_k$, a
contradiction. Following these two lines of argument, it is not difficult to outline a proof scheme by mathematical
induction.

## References
Tom M. Mitchell. 1997. Machine Learning. Chapter 3.

Zhihua Zhou. 2016. Machine Learning. Chapter 4. (In Simplified Chinese)

Hang Li. 2011. Statistical Learning Methods. Chapter 5. (In Simplified Chinese)

{% include links.html %}
